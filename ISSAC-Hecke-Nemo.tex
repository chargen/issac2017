\documentclass{sig-alternate-05-2015}
\usepackage{paralist}
\usepackage{hyperref}
\DeclareMathOperator{\lcm}{lcm}

\begin{document}
\setcopyright{acmcopyright}
\conferenceinfo{ISSAC 2017}{July 25--28, 2017, Kaiserslautern, Germany}

\newtheorem{alg}{Algorithm}
\newtheorem{definition}{Definition}
\newtheorem{Assertion}{Assertion}

\makeatletter
\def\Ddots{\mathinner{\mkern1mu\raise\p@
\vbox{\kern7\p@\hbox{.}}\mkern2mu
\raise4\p@\hbox{.}\mkern2mu\raise7\p@\hbox{.}\mkern1mu}}
\makeatother

%\title{Hecke/Nemo: Number Theory packages for the Julia programming language}
\title{Hecke/Nemo: computer algebra and number theory packages for the Julia programming language}

\numberofauthors{4}
\author{
\alignauthor Claus Fieker\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{fieker@mathematik.uni-kl.de}
\alignauthor William Hart\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{goodwillhart@gmail.com}
\alignauthor Tommy Hofmann\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{thofmann@mathematik.uni-kl.de}
\and
\alignauthor Fredrik Johansson\\
   \affaddr{Inria Bordeaux \&}\\
   \affaddr{Institut de Math\'{e}matiques de Bordeaux} \\
   \affaddr{33400 Talence, France}\\
   \email{fredrik.johansson@gmail.com}
}

\maketitle

\begin{abstract}
We introduce two new packages, Hecke and Nemo, written in the Julia programming language
for computer algebra and number theory.
We demonstrate that high performance generic
algorithms can be implemented in Julia, without the need to resort to a low-level C
implementation. We also describe the various Julia wrappers of existing C/C++ libraries
such as Flint, Arb, Antic and Singular. We give examples of how to use Hecke and Nemo and discuss
some of the algorithms that we have implemented to provide high performance basic
arithmetic.
\end{abstract}


\section{Introduction}

Nemo is a computer algebra package for the Julia programming language. The eventual aim is
to provide highly performant Commutative Algebra, Number Theory and Group Theory routines.

Nemo consists of two parts. The first part consists of wrappers of specialised C/C++
libraries: Flint \cite{flint}, Arb \cite{Arb}, Antic \cite{Antic} and Singular
\cite{singular}.

The second part consists of implementations of generic algorithms and mathematical data
structures in the Julia language. So far the fully recursive constructions include
Univariate polynomial rings, Power series rings, Residue rings (modulo principal ideals),
Fraction fields, Matrices and more recently multivariate polynomials with a sparse
distributed representation.

\subsection{The Julia programming language}

Julia \cite{julia} is a sophisticated, modern programming language which is designe
to be both performant and flexible. It was written for technical computing with a
focus on numerics. However, it has found widespread use for many general purpose
applications, largely due to its innovative type system, dispatch mechanism, JIT
compilation and metaprogramming capabilities.

The first public version of Julia appeared in 2012. 

The benefits of Julia include:

\begin{itemize}
\item Familiar imperative syntax
\item Just-In-Time compilation
\item interactive console
\item Parametric types
\item Powerful metaprogramming facilities
\item Operator overloading
\item Multiple dispatch
\item Efficient native C interface
\item Experimental C++ interface
\item Dynamic type inference
\item Able to be embedded in C programs
\item High performance collection types (dictionaries, iterators, arrays, etc.)
\item Jupyter support (for web based notebooks)
\end{itemize}

Julia's JIT compilation allows fast performance for
generic algorithms. (Contrast: Python?... Alternative: C++ templates?)

\section{Modelling domains in Julia}

Julia provides two levels of types that we make use of abstract types and concrete types.
Concrete types are equivalent to the types that appear in languages such as Java, C or C++,
etc.

Abstract types can be thought of as collections of types. They are used when writing generic
functions that should work for any type in the given collection.

To write a generic function that accepts any type in a given collection of types, we first
create an abstract type. Then we create the individual concrete types that belong to that
abstract type. A generic function can then be constructed with a type parameter, T
say, similar to a template parameter in C++. The main difference is that we can specify
which abstract type our type parameter T must belong to.

In Julia, the symbol <: is used to specify that a given type belongs to a given abstract type.
For example the built-in Julia type Int64 for 64 bit machine integers belongs to the Julia
abstract type Integer.

Abstract types in Julia can form a hierarchy. For example, the Nemo.Field abstract type belongs
to the Nemo.Ring abstract type. An object representing a field in Nemo has type belonging to
Nemo.Field. But because we define the inclusion Nemo.Field <: Nemo.Ring, the type of such an
object also belongs to Nemo.Ring. This means that any generic function in Nemo which is designed
to work with ring objects will also work with field objects.

In Nemo/Hecke we distinguish between the elements of a field, and the field itself, and similarly
for and all other kinds of domains in Nemo. 

The diagram below shows the abstract type hierarchy in Nemo.

\begin{figure}[h]
\centering
\includegraphics[scale=0.37]{types.png}
\caption{The Nemo abstract type hierarchy}
\end{figure}

Naively, one may expect that specific mathematical domains in Nemo/Hecke can be modeled as types
and their elements as objects of the given type. But there are various reasons why this is not a
good model.

As an example, consider the ring $R = \mathbb{Z}/n\mathbb{Z}$. If we were to model the ring $R$
as a type, then the type would need to contain information about the modulus $n$. This is not
possible in Julia if $n$ is an object, e.g. a multiprecision integer.

Putting such information in types is also not desirable. Julia dispatches on type, and each time
we call a generic function with different types, a new version of the function is compiled by the
JIT compiler. This would result in very poor performance if we were writing a multimodular
algorithm, say. In such an algorithm many rings $\mathbb{Z}/n\mathbb{Z}$ may be needed, and
recompilation would be triggered for each distinct $n$.

For this reason, the modulus $n$ needs to be attached to the elements of the ring, not to the type
associated with those elements.

The way we deal with this is to have special (singleton) objects, known as parent objects, that act
like types, but are in fact ordinary Julia objects. As ordinary objects, parents can contain
arbitrary information, such as the modulus in $\mathbb{Z}/n\mathbb{Z}$. Each element in the ring
$\mathbb{Z}/n\mathbb{Z}$ then contains a pointer to the relevant parent object.

This model of mathematical parent objects is taken from SageMath which in turn followed the Magma
computer algebra system.

Julia allows ordinary objects to be made callable. We make use of the facility to write coercions
and constructors for elements of mathematical domains in Nemo/Hecke. For example, the following
code constructs $a = 3 \pmod{7}$.

\begin{verbatim}
R = ResidueRing(ZZ, 7)
a = R(3)
\end{verbatim}

We also make use of the parent object system to encode information such as context objects needed
by C libraries. As Julia objects can have precisely the same bit representation as native C objects,
parent objects can be passed directly to C functions. Additional fields in these objects can safely
appended if it is desirable to retain more information at the Julia level than the C/C++ level. 

\section{Nemo: basic arithmetic in Julia}

\section{Hecke: algebraic number theory in Julia}

\section{Generic algorithms in Nemo}

Many high level algorithms in number theory and computer algebra rely on
the efficient implementation of fundamental algorithms. Here we describe
some of the algorithms implemented in Nemo.

Firstly, we have implemented a number of linear algebra improvements for
matrices over generic commutative rings.

It is well known that for some rings, standard linear algebra algorithms
suffer from coefficient explosion making them impractical to use. Fraction
free algorithms often improve this behaviour, sometimes allowing computations
to be carried out that would essentially never complete with more naive
approaches. For example, fraction free LU decomposition and Gaussian
elimination are well-known.

One place where we have been able to use this strategy is the computation
of the characteristic polynomial. We have adapted the well-known 1937 method
of Danilevsky for computing the characteristic polynomial into a fraction-free
version.

Danilevsky's method works by applying similarity transforms to reduce the
matrix to Frobenius form. Normally such computations are computed over a
field, however each of the outer iterations in the algorithm introduce only
a single denominator. Scaling by this denominator allows us to avoid
fractions.

Of course this then introduces the problem that the entries become larger
as the algorithm proceeds because of the scaling. However, conveniently
it is possible to prove that the introduced scaling factor can be removed 
one step later in the algorithm. This is an exact division and doesn't
lead to denominators.

Removing such a factor has to be done in a way that respects the
similarity transforms. We achieve this by making two passes over the matrix
to remove the common factor.

Next we describe an algorithm we have implemented for efficient
computation of minimal polynomial of an $n\times n$ integer matrix $M$.

A standard approach to this problem is known as ``spinning''. 

We first describe this idea for matrices over a field $K$, where we have
in mind the field $\mathbb{Z}/p\mathbb{Z}$ for a prime $p$.

The main idea is to think of $M$ as a linear operator on an $n$-dimensional
$K$-vector space $V$ and to write $V$ as a sum of invariant subspaces, 
$V = W_1 + W_2 + \cdots + W_k$. If the restriction of $M$ to $W_i$ has
minimum polynomial $f_i$ then the minimum polynomial of $M$ is
$\lcm(f_1, f_2, \ldots, f_k)$.

To generate such subspaces one construct so-called Krylov sequences. Given
a vector $v \in V$ the Krylov subspace associated to $v$ is the linear
subspace spanned by $\{v, Mv, M^2v, \ldots\}$.

One way to get the minimum polynomial of $M$ heuristically is to choose
random vectors $v$, compute the associated Krylov subspaces and take their
minimal polynomials $f_i$. When the least common multiple of such minimal
polynomials stabilises, we have with high probability computed the minimal
polynomial of $M$.

However, a proof of termination can also be given as follows. Suppose that
we have vectors $v_1, v_2, \ldots, v_k$ such that their associated Krylov
subspaces $W_1, W_2, \ldots, W_n$ are pairwise distinct and that their
sum is $V$. Then the least common multiple of their minimal polynomials is
the minimal polynomial of $M$.

Instead of choosing random vectors $v_i$, our algorithm starts with
$v_1 = e_1 = (1, 0, 0, \ldots, 0)$ and at each step chooses $v_i$ to be
the first standard basis vector $e_j$ that is linearly independent of the
Krylov subspaces $W_1, W_2, \ldots, W_{i-1}$ computed so far.

We keep track of which standard basis vectors are independent by
incrementally reducing a matrix $B$ whose rows consist of all the
linearly independent vectors from the Krylov sequences computed so
far.

Any column without a pivot in this reduced matrix $B$ corresponds to a
standard basis vector which is independent of the Krylov subspaces
computed so far.

Once the rank of this incrementally reduced matrix $B$ reaches $n$, the sum
of the Krylov subspaces we have computed is $V$ and we are done.

In order to compute the minimum polynomial of a matrix $M$ over $\mathbb{Z}$,
we make use of a multimodular approach, reducing the matrix $M$ modulo many
small primes $p$ and applying the method described above. We then combine
the minimal polynomials modulo the various primes $p$ using Chinese
remaindering.

The only difficulty is knowing when to stop. Heuristically this happens
when the Chinese remaindering stabilises. Whilst bounds on the number of
primes required do exist, e.g. based on Ovals of Cassini, the bounds are
typically extremely pessimistic. We prefer stop as soon as the Chinese
remaindering stabilises, but we require a proof of termination. 

It is unfortunately way too expensive to simply evaluate the minimum
polynomial $f$ at the matrix $M$ to see if it is zero, since this requires
taking powers of the matrix $M$, which requires $O(n^4)$ operations over
$\mathbb{Z}$ in total, in the worst case.

In order to give a proof of termination, we instead allow a small amount of
information to leak from the modulo $p$ minimal polynomial computations.
Namely, we wish to know which standard basis vectors were used as
generators of the Krylov subspaces modulo $p$. We observe that these are
almost always the same standard basis vectors for all primes $p$ due to the
deterministic way we choose these as the first standard basis vector not
in the sum of the Krylov subspaces computed so far. 

Combining all the generators of the Krylov subspaces for all the primes $p$
then gives us a small set of generators $v_i$ for the Krylov subspaces over
$\mathbb{Z}$ (not necessarily a minimal set).

Suppose that  $f$ is a purported minimum polynomial that is believed
heuristically to be correct, since the Chinese remaindering has stabilised.
On average, it is relatively cheap to compute $f(M)v_i$ for each such
vector $v_i$. If $f(M)v_i = 0$ for each of the vectors $v_i$ then $f$ is
in fact the minimal polynomial.

In the worst case this algorithm still requires $O(n^4)$ operations, but
this is far from the average case, even when the minimal polynomial is not
the characteristic polynomial.

Of course we know that the associated Krylov subspaces $W_i$ sum to $V$
because they do so modulo $p$.

We have been informed by personal communication that the algorithm used by
Magma (\cite{magma}) also uses spinning, however it uses a $p$-adic
approach over $\mathbb{Z}$. Unfortunately we have not been able to find a
publication outlining their method.

In practice our multimodular algorithm seems to just outperform Magma on the
examples we have tried, including matrices with minimum polynomial smaller 
than the characteristic polynomial.

A generic version of this algorithm is implemented in Julia code in Nemo, and
an efficient version over $\mathbb{Z}$ using the Chinese Remaindering trick
is implemented in Flint and made available in Nemo.

We found it extremely convenient to prototype the generic version first in
Nemo. It we found it quite easy to implement a C version from the Julia
implementation.

\section{Specific algorithms in Nemo}

\subsection{Flint: number theory}

We envisioned Flint as a set of implementations for specific rings,
but now we realize that it makes sense to use it only for specific
rings where there is some trick

For example, there seems to be no need to have a Flint type for
matrices over $\left(\mathbb{Z}/n\mathbb{Z}\right)[x]$.

Julia has informed development - for example: don't want to just commit suicide
on bad input

Ground rings $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{Z}/n\mathbb{Z}$,
$\mathbb{F}_q, \mathbb{Q}_p$

dense univariate polynomials, dense matrices
and power series

some flint types require context objects; parent objects hold context objects

algorithms prototyped in Julia, then rewritten in C

showed the way for multivariate polynomials 

\subsection{Arb: arbitrary precision ball arithmetic}

Viable approaches to represent elements of $\mathbb{R}$ and $\mathbb{C}$
include floating-point approximations,
intervals, and lazy bit streams (e.g.\ using a finite-precision
approximation together with an exact symbolic DAG
to allow re-computing the approximation to higher precision).

Nemo includes wrapper code for Arb, which implements real numbers as
arbitrary-precision midpoint-radius intervals (balls) $[m \pm r]$
and complex numbers as rectangular boxes $[a \pm r_1]$ + $[b \pm r_2] i$.

While alternative implementations of $\mathbb{R}$ and $\mathbb{C}$
may be added to Nemo in the future, we envision Arb as a reasonable default,
and we have used it with good success (see Hecke).

For many computer algebra algorithms, the error analysis
necessary to guarantee correct results with floating-point arithmetic
becomes impractical to do by hand.
Interval arithmetic solves this problem by effectively making
error analysis automatic.

With arbitrary-precision interval arithmetic, a form of
coarse-grained lazy evaluation is possible: the user can
try a computation with some tentative precision $p$ and restart
with precision $2p$ if that fails. The precision can be set
optimally when a good estimate for the minimal
required~$p$ is available; that is, the intervals
can be used as if they were plain floating-point numbers, and the automatic
error bounds simply provide a certificate.

A lazy representation for single numbers would be more convenient in some
applications,
but also probably slower (and less space-efficient) due to storing DAGs.
Implementing lazy numbers on top of Arb in Julia
would be an interesting future project.

The midpoint-radius representation used by Arb is particularly efficient
at high precision,
since low precision is sufficient for the radii, which means that
speed and memory efficiency is close to the optimum possible
with plain arbitrary-precision floating-point arithmetic.
Like Flint, Arb systematically uses asymptotically fast algorithms
for operations such as polynomial multiplication, with tuning
for different problem sizes.


ArbField, AcbField

polynomials, matrices

also transcendental functions

\subsection{Antic: algebraic number fields}

\subsection{Singular: commutative algebra}

\section{Future directions}

\section{Acknowledgement}


\begin{thebibliography}{99}

\bibitem{magma}
J. J. Cannon, W. Bosma (Eds.) {\em Handbook of Magma Functions}, Edition 2.13 (2006)

\bibitem{mca} 
J. von zur Gathen and J. Gerhard. {\em Modern Computer Algebra}. Cambridge University Press, 1999.

\bibitem{flint}
W. Hart, F. Johansson and S. Pancratz. {\em FLINT}, open-source C-library. \url{http://www.flintlib.org}

\bibitem{arb}
F. Johansson. {\em Arb}, open-source C-library. \url{http://arblib.org}

\bibitem{ntl}
V. Shoup {\em NTL}, open-source C++ library. \url{http://www.shoup.net/ntl/}

\bibitem{sage}
W. Stein {\em SAGE Mathematics Software}.  \url{http://www.sagemath.org}

\bibitem{julia} J. Bezanson, A. Edelman, S. Karpinski and V. B. Shah. {\em Julia: A fresh approach to numerical computing}. \url{https://arxiv.org/abs/1411.1607}

\bibitem{singular} W. Decker, G. M. Greuel, G. Pfister and H. Sch\"onemann. {\em Singular} --- A computer algebra system for polynomial computations. \url{http://www.singular.uni-kl.de}

\end{thebibliography}
\end{document}
