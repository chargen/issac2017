\documentclass{sig-alternate-05-2015}
\usepackage{paralist}
\usepackage{url}
\usepackage{hyperref}
\DeclareMathOperator{\lcm}{lcm}

\begin{document}
\setcopyright{acmcopyright}
\conferenceinfo{ISSAC 2017}{July 25--28, 2017, Kaiserslautern, Germany}

\newtheorem{alg}{Algorithm}
\newtheorem{definition}{Definition}
\newtheorem{Assertion}{Assertion}

\makeatletter
\def\Ddots{\mathinner{\mkern1mu\raise\p@
\vbox{\kern7\p@\hbox{.}}\mkern2mu
\raise4\p@\hbox{.}\mkern2mu\raise7\p@\hbox{.}\mkern1mu}}
\makeatother

\title{Nemo/Hecke: computer algebra and number theory packages for the Julia programming language}

\numberofauthors{4}
\author{
\alignauthor Claus Fieker\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{fieker@mathematik.uni-kl.de}
\alignauthor William Hart\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{goodwillhart@gmail.com}
\alignauthor Tommy Hofmann\\
   \affaddr{TU Kaiserslautern}\\
   \affaddr{Fachbereich Mathematik, Postfach 3049,}\\
   \affaddr{67653 Kaiserslautern, Germany}\\
   \email{thofmann@mathematik.uni-kl.de}
\and
\alignauthor Fredrik Johansson\\
   \affaddr{Inria Bordeaux \&}\\
   \affaddr{Institut de Math\'{e}matiques de Bordeaux} \\
   \affaddr{33400 Talence, France}\\
   \email{fredrik.johansson@gmail.com}
}

\maketitle

\begin{abstract}
We introduce two new packages, Nemo and Hecke, written in the Julia programming language
for computer algebra and number theory.
We demonstrate that high performance generic
algorithms can be implemented in Julia, without the need to resort to a low-level C
implementation. We also describe the various Julia wrappers of existing C/C++ libraries
such as Flint, Arb, Antic and Singular. We give examples of how to use Hecke and Nemo and discuss
some of the algorithms that we have implemented to provide high performance basic
arithmetic.
\end{abstract}


\section{Introduction}

Nemo is a computer algebra package for the Julia programming language. The eventual aim is
to provide highly performant commutative algebra, number theory and group theory routines.

Nemo consists of two parts: wrappers of specialised C/C++
libraries (Flint \cite{flint}, Arb \cite{arb}, Antic \cite{antic} and Singular
\cite{singular}), and
implementations of generic algorithms and mathematical data
structures in the Julia language. So far the fully recursive constructions include
univariate polynomial rings, power series rings, residue rings (modulo principal ideals),
fraction fields, matrices and more recently multivariate polynomials with a sparse
distributed representation.

\subsection{The Julia programming language}

Julia \cite{julia} is a sophisticated, modern programming language which is designed
to be both performant and flexible.
The first public version of Julia appeared in 2012. 
Julia was written for technical computing with a
focus on numerics. However, it has found widespread use for many general purpose
applications, largely due to its innovative type system, dispatch mechanism, JIT
compilation and metaprogramming capabilities.
The benefits of Julia include:

\begin{itemize}
\item Familiar imperative syntax (like Python and Matlab), including support for operator overloading.
\item An interactive console, as well as Jupyter support (for web based notebooks).
\item Parametric types, multiple dispatch, and powerful metaprogramming facilities.
\item Just-In-Time compilation, supported by dynamic type inference.
\item Automatic memory management (garbage collection).
\item An efficient native C interface, and more recently an experimental C++ interface.
\item High performance collection types (dictionaries, iterators, arrays, etc.) and a powerful standard library.
\end{itemize}

Julia's JIT compilation allows fast performance for
generic algorithms. (Contrast: Python?... Alternative: C++ templates?)

\section{Modelling domains in Julia}

Julia provides two levels of types that we make use of abstract types and concrete types.
Concrete types are equivalent to the types that appear in languages such as Java, C or C++,
etc.

Abstract types can be thought of as collections of types. They are used when writing generic
functions that should work for any type in the given collection.

To write a generic function that accepts any type in a given collection of types, we first
create an abstract type. Then we create the individual concrete types that belong to that
abstract type. A generic function can then be constructed with a type parameter, T
say, similar to a template parameter in C++. The main difference is that we can specify
which abstract type our type parameter T must belong to.

In Julia, the symbol <: is used to specify that a given type belongs to a given abstract type.
For example the built-in Julia type Int64 for 64 bit machine integers belongs to the Julia
abstract type Integer.

Abstract types in Julia can form a hierarchy. For example, the Nemo.Field abstract type belongs
to the Nemo.Ring abstract type. An object representing a field in Nemo has type belonging to
Nemo.Field. But because we define the inclusion Nemo.Field <: Nemo.Ring, the type of such an
object also belongs to Nemo.Ring. This means that any generic function in Nemo which is designed
to work with ring objects will also work with field objects.

Julia always picks the most specific function that applies to a given type. This allows one to
implement a function at the most general level of a type hierarchy at which it applies. One can
also write a version of a given function for specific concrete types. For example, one may wish
to call a specific C implementation for a multiplication algorithm, say, when arguments with a
certain very specific given type are passed.

Another way that we make use of Julia's abstract type system in Nemo/Hecke is to distinguish
between the type of elements of fields, and the fields themselves, and similarly for all other
kinds of domains in Nemo. 

The diagram below shows the abstract type hierarchy in Nemo.

\begin{figure}[h]
\centering
\includegraphics[scale=0.37]{types.png}
\caption{The Nemo abstract type hierarchy}
\end{figure}

Naively, one may expect that specific mathematical domains in Nemo/Hecke can be modeled as types
and their elements as objects of the given type. But there are various reasons why this is not a
good model.

As an example, consider the ring $R = \mathbb{Z}/n\mathbb{Z}$. If we were to model the ring $R$
as a type, then the type would need to contain information about the modulus $n$. This is not
possible in Julia if $n$ is an object, e.g. a multiprecision integer.

Putting such information in types is also not desirable. Julia dispatches on type, and each time
we call a generic function with different types, a new version of the function is compiled by the
JIT compiler. This would result in very poor performance if we were writing a multimodular
algorithm, say. In such an algorithm many rings $\mathbb{Z}/n\mathbb{Z}$ may be needed, and
recompilation would be triggered for each distinct $n$.

For this reason, the modulus $n$ needs to be attached to the elements of the ring, not to the type
associated with those elements.

The way we deal with this is to have special (singleton) objects, known as parent objects, that act
like types, but are in fact ordinary Julia objects. As ordinary objects, parents can contain
arbitrary information, such as the modulus in $\mathbb{Z}/n\mathbb{Z}$. Each element in the ring
$\mathbb{Z}/n\mathbb{Z}$ then contains a pointer to the relevant parent object.

This model of mathematical parent objects is taken from SageMath which in turn followed the Magma
computer algebra system.

Julia allows ordinary objects to be made callable. We make use of the facility to write coercions
and constructors for elements of mathematical domains in Nemo/Hecke. For example, the following
code constructs $a = 3 \pmod{7}$.

\begin{verbatim}
R = ResidueRing(ZZ, 7)
a = R(3)
\end{verbatim}

We also make use of the parent object system to encode information such as context objects needed
by C libraries. As Julia objects can have precisely the same bit representation as native C objects,
parent objects can be passed directly to C functions. Additional fields in these objects can safely
appended if it is desirable to retain more information at the Julia level than the C/C++ level. 

\section{Nemo: basic arithmetic in Julia}

Julia was designed with high performance numerical algorithms in mind, and can be used to
implement numerical algorithms involving floating point numbers at essentially the same
speed as a low-level C or Fortran algorithm. 

It is remarkable that the performance of Julia is so close to C in the optimum case that
it is possible to implement fast basic arithmetic in Julia itself. We view the main
advantage of Julia as the ability to implement fast generic algorithms, for which a specific
implementation does not exist in C. But it is also quite convenient to prototype low level
algorithms in Julia and then port this to low level, hand optimised C with manual memory
management at a later stage.

Of course, many algebraic applications depend on fast assembly optimised bignum arithmetic
which Julia is unlikely to be able to compete with. In fact, Julia itself wraps the GMP
\cite{gmp} and MPFR \cite{mpfr} libraries for its own multiprecision arithmetic.

We give a few benchmark problems as code examples.
Additional benchmarks are available
at \url{http://nemocas.org/benchmarks.html}.

The following example computes a resultant
in the ring $((\operatorname{GF}(17^{11})[y])/(y^3 + 3xy + 1))[z]$,
demonstrating generic recursive rings and polynomial
arithmetic. Flint is used for univariate polynomials over a finite field.

\begin{verbatim}
R, x = FiniteField(7, 11, "x")
S, y = PolynomialRing(R, "y")
T = ResidueRing(S, y^3 + 3x*y + 1)
U, z = PolynomialRing(T, "z")

f = (3y^2+y+x)*z^2 + ((x+2)*y^2+x+1)*z + 4x*y + 3
g = (7y^2-y+2x+7)*z^2 + (3y^2+4x+1)*z + (2x+1)*y + 1

s = f^12
t = (s + g)^12
resultant(s, t)
\end{verbatim}

This example takes 179907~s in SageMath~6.8, 82 s in Magma V2.21-4
and 0.2 s in Nemo-0.4.

Next, we compute the determinant of a random $80\times80$ matrix
with entries in a cubic number field.
This benchmark is designed to test generic linear algebra over a field
where there is coefficient blowup. The number field arithmetic is
provided by Antic.

\begin{verbatim}
QQx, x = PolynomialRing(QQ, "x")
K, a = NumberField(x^3 + 3*x + 1, "a")
M = MatrixSpace(K, 80, 80)()

for i in 1:80
 for j in 1:80
   for k in 0:2
     M[i, j] = M[i, j] + a^k * (rand(-100:100))
   end
 end
end

det(M)
\end{verbatim}

This takes 5893~s in SageMath~6.8, 21.9~s in Pari/GP~2.7.4, 5.3~s in Magma V2.21-4,
and 2.4~s in Nemo-0.4.

\section{Generic algorithms in Nemo}

Many high level algorithms in number theory and computer algebra rely on
the efficient implementation of fundamental algorithms.

\subsection{Generic functions}

Maybe talk about how they are defined and how type dispatch works here?

\subsection{In-place operations}

A consistent way to improve performance is to modify
elements in-place, especially when making incremental updates
to large, highly nested objects.
In-place operations are used pervasively in Flint (and one level below, in GMP/MPIR).
Nemo objects are ostensibly immutable, but local mutation is
acceptable while an object is being created,
and Nemo provides special methods for this purpose.
For recursively constructed objects, the in-place methods work
recursively down to and including the ground level C objects.
Thus, instead of writing
\begin{verbatim}
s += a * b
\end{verbatim}
which creates an object for \texttt{a * b}
and then replaces \texttt{s} with yet another new object, we may
create a scratch variable \texttt{t} with the same type as \texttt{a}, \texttt{b} and \texttt{s} and use
\begin{verbatim}
mul!(t, a, b)
addeq!(s, t)
\end{verbatim}
which creates no new objects and avoids making a copy of the data in \texttt{s}
that is not being modified.
Naturally, the idea is to limit this less readable construct to
critical inner loops of operations such as polynomial
and matrix multiplication.
Using this in-place update in the main loop of univariate polynomial
multiplication saves a whole factor 6.7 on the Fateman benchmark
$f \cdot (f+1)$ where
$f = (1 + x + y + z + t)^{30}$, evaluated over the nested univariate
ring $\mathbb{Z}[x][y][z][t]$. This is an extreme example, and
the typical gain is more modest.

\subsection{Polynomial algorithms}

For dense univariate polynomials over a commutative ring we implement classical
polynomial multiplication. However, when the base ring is itself a polynomial ring,
we have available an implementation of Kronecker segmentation, to reduce the
multivariate multiplication to the univariate case.

For use in the subresultant GCD algorithm, we implement generic, polynomial
pseudoremainder. 

The generic polynomial resultant code computes the resultant using subresultant
pseudoremainder sequences. This code can even be called for polynomials over a
ring with zero divisors, so long as impossible inverses are caught. The exception
can be caught using a try/catch block in Julia and a fallback method called, e.g.
the resultant can be computed using Sylvester matrices. This allows an algorithm
with quadratic complexity to be called generically, with an algorithm of cubic
complexity only called as backup in case of failure.

For generic power series, Nemo implements both relative and absolute
representations. In the relative case, the valuation is stored separately so that
the internal representation begins with the first nonzero coefficient (for nonzero
power series). This greatly speeds up working with power series that have large
valuation.

For powering we usually make use of the standard square-and-add approach. However
when a polynomial with few terms is raised to a high power, we make use of the
multinomial powering algorithm.

Nemo also provides a module for generic multivariate polynomials in sparse distributed
format. We made use of variants of the sparse algorithms described by Monagan and
Pearce for heap-based multiplication \cite{heapmul}, exact division and division with
remainder \cite{heapdiv} and powering \cite{heappow}.

In the case of pseudodivision for sparse, multivariate polynomials, we extract one
of the variables and then implement a heap-based, univariate pseudoremainder
algorithm inspired by an unpublished manuscript of Monagan and Pearce.

We make use of this pseudoremainder implementation to implement the subresultant
GCD algorithm. To speed this up, we make use of a number of tricks employed by the
Giac/XCAS system \cite{giac}, as taught to us by Bernard Parisse. The most important
of these is cheap removal of the content that accumulates during the subresultant
algorithm by analysing the input polynomials to see what form the leading coefficient
of the GCD can take.

We also use heuristics to determine which permutation of the variables will lead to
the GCD being computed in the fastest time. This heuristic favours the variable with
the lowest degree as the main variable for the computation, with the other variables 
following in increasing order of degree thereafter. But our heuristic heavily favours
variables in which the polynomial is monic.

\subsection{Matrix algorithms}

Here we describe some of the linear algebra improvements and algorithms for matrices
over generic commutative rings that we have implemented in Nemo.

For computing the determinant over a generic commutative ring, we implemented a generic
algorithm making use of Clow sequences \cite{clow} which uses only $O(n^4)$ ring operations
in the dimension $n$ of the matrix. However, two other approaches seem to always outperform
it. The first approach makes use of a generic fraction free LU decomposition. This algorithm
may fail if an impossible inverse is encountered. However, as a fallback, we make use of a
division free determinant algorithm. This computes the characteristic polynomial and then
extracts the determinant from that.

For determinants of matrices over polynomial rings, we use an interpolation approach.

We implemented an algorithm for computing the characteristic polynomial due to
Danilevsky (see below) and an algorithm that is based on computing the Hessenberg
form. We also make use of a generic implementation of the algorithm of Berkowitz which
is division free. 

As is well known, fraction free algorithms often improve the performance of matrix
algorithms over an integral domain. For example, fraction free Gaussian elimination
for LU decomposition, inverse, determinant and reduced row echelon form
are well-known.

One place where we have been able to use this strategy is the computation
of the characteristic polynomial. We have adapted the well-known 1937 method
of Danilevsky for computing the characteristic polynomial into a fraction-free
version.

Danilevsky's method works by applying similarity transforms to reduce the
matrix to Frobenius form. Normally such computations are computed over a
field, however each of the outer iterations in the algorithm introduce only
a single denominator. Scaling by this denominator allows us to avoid
fractions.

Of course this then introduces the problem that the entries become larger
as the algorithm proceeds because of the scaling. However, conveniently
it is possible to prove that the introduced scaling factor can be removed 
one step later in the algorithm. This is an exact division and doesn't
lead to denominators.

Removing such a factor has to be done in a way that respects the
similarity transforms. We achieve this by making two passes over the matrix
to remove the common factor.

\subsubsection{Minimal polynomial}

Next we describe an algorithm we have implemented for efficient
computation of minimal polynomial of an $n\times n$ integer matrix $M$.

A standard approach to this problem is known as ``spinning''. 
We first describe this idea for matrices over a field~$K$, where we have
in mind the field $\mathbb{Z}/p\mathbb{Z}$ for a prime $p$.

The main idea is to think of $M$ as a linear operator on an $n$-dimensional
$K$-vector space $V$ and to write $V$ as a sum of invariant subspaces, 
$V = W_1 + W_2 + \cdots + W_k$. If the restriction of $M$ to $W_i$ has
minimal polynomial $f_i$ then the minimal polynomial of $M$ is
$\lcm(f_1, f_2, \ldots, f_k)$.

To generate such subspaces we construct so-called Krylov sequences. Given
a vector $v \in V$ the Krylov subspace associated to $v$ is the linear
subspace spanned by $\{v, Mv, M^2v, \ldots\}$.

One way to get the minimal polynomial of $M$ heuristically is to choose
random vectors $v$, compute the associated Krylov subspaces and take their
minimal polynomials $f_i$. When the least common multiple of such minimal
polynomials stabilises, we have with high probability computed the minimal
polynomial of $M$.

However, a proof of termination can also be given as follows. Suppose that
we have vectors $v_1, v_2, \ldots, v_k$ such that their associated Krylov
subspaces $W_1, W_2, \ldots, W_n$ are pairwise distinct and that their
sum is $V$. Then the least common multiple of their minimal polynomials is
the minimal polynomial of $M$.

Instead of choosing random vectors $v_i$, our algorithm starts with
$v_1 = e_1 = (1, 0, 0, \ldots, 0)$ and at each step chooses $v_i$ to be
the first standard basis vector $e_j$ that is linearly independent of the
Krylov subspaces $W_1, W_2, \ldots, W_{i-1}$ computed so far.

We keep track of which standard basis vectors are independent by
incrementally reducing a matrix $B$ whose rows consist of all the
linearly independent vectors from the Krylov sequences computed so
far.

Any column without a pivot in this reduced matrix $B$ corresponds to a
standard basis vector which is independent of the Krylov subspaces
computed so far.

Once the rank of this incrementally reduced matrix $B$ reaches $n$, the sum
of the Krylov subspaces we have computed is $V$ and we are done.

In order to compute the minimal polynomial of a matrix $M$ over $\mathbb{Z}$,
we make use of a multimodular approach, reducing the matrix $M$ modulo many
small primes $p$ and applying the method described above. We then combine
the minimal polynomials modulo the various primes $p$ using Chinese
remaindering.

The only difficulty is knowing when to stop. Heuristically this happens
when the Chinese remaindering stabilises. Whilst bounds on the number of
primes required do exist, e.g. based on Ovals of Cassini, the bounds are
typically extremely pessimistic. We prefer stop as soon as the Chinese
remaindering stabilises, but we need a proof of termination. 

It is unfortunately way too expensive to simply evaluate the minimal
polynomial $f$ at the matrix $M$ to see if it is zero, since this requires
taking powers of the matrix $M$, which requires $O(n^4)$ operations over
$\mathbb{Z}$ in total, in the worst case.

In order to give a proof of termination, we instead allow a small amount of
information to leak from the modulo~$p$ minimal polynomial computations.
Namely, we wish to know which standard basis vectors were used as
generators of the Krylov subspaces modulo~$p$. We observe that these are
almost always the same standard basis vectors for all primes~$p$ due to the
deterministic way we choose these as the first standard basis vector not
in the sum of the Krylov subspaces computed so far. 

Combining all the generators of the Krylov subspaces for all the primes~$p$
then gives us a small set of generators $v_i$ for the Krylov subspaces over
$\mathbb{Z}$ (not necessarily a minimal set).

Suppose that  $f$ is a purported minimal polynomial that is believed
heuristically to be correct, since the Chinese remaindering has stabilised.
On average, it is relatively cheap to compute $f(M)v_i$ for each such
vector $v_i$. If $f(M)v_i = 0$ for each of the vectors $v_i$ then $f$ is
in fact the minimal polynomial.

In the worst case this algorithm still requires $O(n^4)$ operations, but
this is far from the average case, even when the minimal polynomial is not
the characteristic polynomial.

Of course we know that the associated Krylov subspaces $W_i$ sum to $V$
because they do so modulo $p$.

We have been informed by personal communication that the algorithm used by
Magma (\cite{magma}) also uses spinning, however it uses a $p$-adic
approach over $\mathbb{Z}$. Unfortunately we have not been able to find a
publication outlining their method.

In practice our multimodular algorithm seems to just outperform Magma on the
examples we have tried, including matrices with minimal polynomial smaller 
than the characteristic polynomial.

A generic version of this algorithm is implemented in Julia code in Nemo, and
an efficient version over $\mathbb{Z}$ using the Chinese remaindering trick
is implemented in Flint and made available in Nemo.
We found it extremely convenient to prototype the generic version first in
Nemo. We then found it quite easy to implement a C version from the Julia
implementation.

\section{Hecke: algebraic number theory in Julia}

\section{C/C++ wrappers in Nemo}

\subsection{Flint: arithmetic and number theory}

Flint provides arithmetic over
$\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{Z}/n\mathbb{Z}$,
$\operatorname{GF}(q), \mathbb{Q}_p$
as well as matrices, polynomials and power series over most
of these ground rings.
Nemo implements elements of these rings as thin wrappers of the
Flint C types.
Some Flint types such as finite fields
require context objects, which we
store in the corresponding parent objects.

Nemo uses Flint types where available instead of generic code.
For instance, \texttt{PolynomialRing(ZZ, "x")} automatically uses
the Julia wrapper of Flint's \texttt{fmpz\_poly} instead of generic
polynomials over Nemo's \texttt{ZZ} which wraps Flint's \texttt{fmpz}
integer type.
The benefit is that
Flint implements many low level tricks and algorithmic optimisations
specific to each ring. For example, Flint's multiplication in $\mathbb{Z}[x]$
uses a basecase method with optimisations for small coefficients,
Karatsuba multiplication, Kronecker segmentation and
a Sch\"{o}nhage-Strassen FFT, with accurate cutoffs
between algorithms depending on the degrees and coefficient sizes.

Additional Flint algorithms wrapped by Nemo include
primality testing, polynomial factorisation, LLL, and Smith and Hermite
normal forms of integer matrices.

\subsection{Arb: arbitrary precision ball arithmetic}

Computing over $\mathbb{R}$ and $\mathbb{C}$ requires
using some approximate model for these fields.
Viable approaches include floating-point approximations,
intervals, and lazy bit streams (e.g.\ using a finite-precision
approximation together with an exact symbolic DAG
to allow re-computing the approximation to higher precision).

Nemo includes wrapper code for Arb, which implements real numbers as
arbitrary-precision midpoint-radius intervals (balls) $[m \pm r]$
and complex numbers as rectangular boxes $[a \pm r_1]$ + $[b \pm r_2] i$.
The precision is stored in the parent object, e.g.\ \texttt{R = ArbField(53)}
creates a field of Arb real numbers with 53-bit precision.
Arb also supplies types for polynomials, power series and matrices
over $\mathbb{R}$ and $\mathbb{C}$, as well as transcendental functions.

While alternative implementations of $\mathbb{R}$ and $\mathbb{C}$
may be added to Nemo in the future, we envision Arb as a reasonable default,
and we have used it with good success (see Hecke).

For many computer algebra algorithms, the error analysis
necessary to guarantee correct results with floating-point arithmetic
becomes impractical to do by hand.
Interval arithmetic solves this problem by effectively making
error analysis automatic.

With arbitrary-precision interval arithmetic, a form of
coarse-grained lazy evaluation is possible: the user can
try a computation with some tentative precision $p$ and restart
with precision $2p$ if that fails. The precision can be set
optimally when a good estimate for the minimal
required~$p$ is available; that is, the intervals
can be used as if they were plain floating-point numbers, and the automatic
error bounds simply provide a certificate.

A lazy representation for single numbers would be more convenient in some
applications,
but also probably slower (and less space-efficient) due to storing DAGs.
Implementing lazy numbers on top of Arb in Julia
would be an interesting future project.

The midpoint-radius representation used by Arb is particularly efficient
at high precision,
since low precision is sufficient for the radii, which means that
speed and memory efficiency is close to the optimum possible
with plain arbitrary-precision floating-point arithmetic.
Like Flint, Arb systematically uses asymptotically fast algorithms
for operations such as polynomial multiplication, with tuning
for different problem sizes.

\subsection{Antic: algebraic number fields}

Antic (Algebraic Number Theory in C) is a C library, depending on the Flint library, for
doing computations in algebraic number fields. At this stage it contains algebraic number
field element arithmetic only.

Nemo makes use of Antic for speeding up element arithmetic. We briefly describe some
of the techniques Antic uses for fast arithmetic, and how Nemo interfaces with Antic, but
refer the reader to the article \cite{anticrundbrief} for full details.

Antic represents number field elements as Flint polynomials and derives much of its speed
from the highly optimised polynomial arithmetic in Flint. However, there are a few
additional tricks employed for number fields.

Firstly, Antic makes use of number field context objects, which are set up when a number
field is created. The results of a number of precomputations are stored in the context
object to speed up subsequent computations with number field elements. Number field parent
objects at the Nemo level consist precisely of these context objects.

The first precomputation is a precomputed inverse of the leading coefficient of the defining
polynomial of the number field. This helps speed up reduction modulo the defining polynomial.

The second piece of precomputed data is an array of precomputed powers $x^i$ modulo the
defining polynomial $f(x)$. This allows fast reduction of polynomials whose degree exceeds
that of the defining polynomial, e.g. in multiplication of number field elements $g$ anf $h$
where one wants to compute $g(x)h(x) \pmod{f(x)}$.

The third precomputation is of traces $S_k = \sum_{i=1}^n \theta_i^k$ where the $\theta_i$
are the roots of the defining polynomial $f(x)$. These traces are precomputed using
recursive formulae as described for example in \cite{cohen}. These precomputed traces are
used to speed up the computation of traces of elements of the algebraic number field.

Norms of elements of number fields are computed using resultants, for which we make use of
the fast polynomial resultant code in Flint, rather than an expensive Sylvester matrix
computation.

Inverses of elements are likewise computed using very fast polynomial extended GCD code in
Flint.

Antic also offers the ability to do multiplication of number field elements without
reduction. This is useful for speeding up the dot products that occur in matrix
multiplication, for example. Instead of reducing after every multiplication, the unreduced
products are first accumulated and their sum can be reduced at the end of the dot product.

To facilitate this delayed reduction, all Antic number field elements are allocated with
sufficient space to store a single product, rather the reduction of such a product.

\subsection{Singular: commutative algebra}

We briefly mention the preliminary work of our colleague Oleksandr Motsak, who has written
a Singular wrapper in Julia.

Singular \cite{singular} is a computer algebra system written in C++ for commutative algebra
and geometry, with a focus on singularity theory. Julia is able to inteface to C++ using
the experimental Cxx.jl package, which makes use of the Clang compiler infrastructure to
compile arbitrary C++ down to the same LLVM bytecode as the Julia compiler. This allows
for the inclusion of arbitrary inline C++ in Julia functions, including template 
instantiation at runtime.

The prototype Singular wrapper has been turned into a Julia package called Singular.jl
\cite{singular.jl}. It provides direct access to the Singular kernel, including its
Groebner engine and allows both Singular polynomials to be defined over Nemo rings and Nemo
generic rings to be defined over Singular coefficient rings and polynomial rings.

\section{Future directions}

\section{Acknowledgement}


\begin{thebibliography}{99}

\bibitem{magma}
J. J. Cannon, W. Bosma (Eds.) {\em Handbook of Magma Functions}, Edition 2.13 (2006)

\bibitem{mca} 
J. von zur Gathen and J. Gerhard. {\em Modern Computer Algebra}. Cambridge University Press, 1999.

\bibitem{flint}
W. Hart, F. Johansson and S. Pancratz. {\em FLINT}, open-source C-library. \url{http://www.flintlib.org}

\bibitem{antic}
W. Hart, {\em ANTIC: Algebraic Number Theory in C}, Computeralgebra Rundbrief 56, Fachgruppe Computeralgebra, 2015. \url{http://www.fachgruppe-computeralgebra.de/data/CA-Rundbrief/car56.pdf}

\bibitem{arb}
F. Johansson. {\em Arb}, open-source C-library. \url{http://arblib.org}

\bibitem{ntl}
V. Shoup {\em NTL}, open-source C++ library. \url{http://www.shoup.net/ntl/}

\bibitem{sage}
W. Stein {\em SageMath Mathematics Software}.  \url{http://www.sagemath.org}

\bibitem{julia} J. Bezanson, A. Edelman, S. Karpinski and V. B. Shah. {\em Julia: A fresh approach to numerical computing}. \url{https://arxiv.org/abs/1411.1607}

\bibitem{singular} W. Decker, G. M. Greuel, G. Pfister and H. Sch\"onemann. {\em Singular} --- A computer algebra system for polynomial computations. \url{http://www.singular.uni-kl.de}

\end{thebibliography}
\end{document}

